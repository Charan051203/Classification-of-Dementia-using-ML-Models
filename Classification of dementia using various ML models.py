# -*- coding: utf-8 -*-
"""Dementia Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCzOLYkbxRd-HRBMjwzIIlL3W15FcExk

# **Preprocessing Data**
"""

import pandas as pd
import numpy as np
dementia = pd.read_csv("/content/dementia_dataset.csv")
dementia.head()

X=dementia.drop('Group',axis=1)
y=dementia['Group']

print(X)

print(y)

pd.DataFrame(X, columns=dementia.columns[:-1])

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=0)

"""# **ML Models**

**Logistic regression**
"""

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(random_state=16)
logreg.fit(X_train,y_train)
y_pred_logreg = logreg.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))
print(accuracy_score(y_test, y_pred_logreg))

"""**KNN**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train,y_train)
y_pred_knn = knn.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))
print(accuracy_score(y_test, y_pred_knn))

"""**Support Vector Machine**"""

from sklearn import svm
clf = svm.SVC(kernel='linear', C=0.01)
clf.fit(X_train,y_train)
y_pred_svm = clf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
print(accuracy_score(y_test, y_pred_svm))
print(y_pred_svm)

#confusion matrix
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import confusion_matrix
cm=metrics.confusion_matrix(y_test, y_pred_svm, labels=[1, 0])
df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 1","Actual 0"]],
 columns = [i for i in ["Predict 1","Predict 0"]])
sns.heatmap(df_cm, annot=True, fmt='g')

from sklearn import svm
clf = svm.SVC(kernel='poly', degree=5)
clf.fit(X_train,y_train)
y_pred_svm_poly = clf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_svm_poly))
print(classification_report(y_test, y_pred_svm_poly))
print(accuracy_score(y_test, y_pred_svm_poly))

from sklearn import svm
clf = svm.SVC(kernel='rbf', degree=5)
clf.fit(X_train,y_train)
y_pred_svm_rbf = clf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_svm_rbf))
print(classification_report(y_test, y_pred_svm_rbf))
print(accuracy_score(y_test, y_pred_svm_rbf))

"""**Decision Tree**"""

from sklearn.tree import DecisionTreeClassifier
dTree = DecisionTreeClassifier()
dTree.fit(X_train,y_train)
y_pred_dTree = dTree.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_dTree))
print(classification_report(y_test, y_pred_dTree))
print(accuracy_score(y_test, y_pred_dTree))

"""# **Bagging**

**Random Forest Classifier**
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=150)
rf.fit(X_train,y_train)
y_pred_rf = rf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print(accuracy_score(y_test, y_pred_rf))

#confusion matrix
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import confusion_matrix
cm=metrics.confusion_matrix(y_test, y_pred_rf, labels=[1, 0])
df_cm = pd.DataFrame(cm, index = [i for i in ["Actual 1","Actual 0"]],
 columns = [i for i in ["Predict 1","Predict 0"]])
sns.heatmap(df_cm, annot=True, fmt='g')

from sklearn.preprocessing import LabelEncoder
from sklearn import svm
le=LabelEncoder()
meta_data=np.concatenate((y_pred_logreg, y_pred_knn, y_pred_svm, y_pred_svm_poly, y_pred_svm_rbf, y_pred_dTree, y_pred_rf, y_test))
le.fit(meta_data)
X=np.array([y_pred_logreg, y_pred_knn, y_pred_svm, y_pred_svm_poly, y_pred_svm_rbf, y_pred_dTree, y_pred_rf]).T
meta_learner= svm.SVC()
meta_learner.fit(X,y_test)
ensemble_predicitions=meta_learner.predict(X)
from sklearn.metrics import confusion_matrix,classification_report, accuracy_score
print(confusion_matrix(y_test, ensemble_predicitions))
print(classification_report(y_test, ensemble_predicitions))
print(accuracy_score(y_test, ensemble_predicitions))

"""# **Models vs Accuracy**"""

import numpy as np
import matplotlib.pyplot as plt
data = {'SVM': 81, 'LR':83, 'k-NN':55,
		'DT':81,'RF':90,'RSCV':90,'GSCV':91}
Model = list(data.keys())
Accuracy = list(data.values())
fig = plt.figure(figsize = (10, 5))
plt.bar(Model,Accuracy, color ='cyan',
		width = 0.4)
plt.xlabel("Models")
plt.ylabel("Accuracy in percentage")
plt.title("Model and its accuracy")
plt.show()

"""# **Hyper Parameter Tuning**"""

model=RandomForestClassifier(n_estimators=300,criterion='entropy',
                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)
predictions=model.predict(X_test)
print(confusion_matrix(y_test,predictions))
print(accuracy_score(y_test,predictions))
print(classification_report(y_test,predictions))

"""**Randomized Search CV**"""

import numpy as np
from sklearn.model_selection import RandomizedSearchCV
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
max_features = ['auto', 'sqrt','log2']
max_depth = [int(x) for x in np.linspace(10, 1000,10)]
min_samples_split = [2, 5, 10,14]
min_samples_leaf = [1, 2, 4,6,8]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              'criterion':['entropy','gini']}
print(random_grid)

rf=RandomForestClassifier()
rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,
                               random_state=100,n_jobs=-1)
rf_randomcv.fit(X_train,y_train)

rf_randomcv.best_params_

rf_randomcv

best_random_grid=rf_randomcv.best_estimator_

from sklearn.metrics import accuracy_score
y_pred=best_random_grid.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print("Accuracy Score {}".format(accuracy_score(y_test,y_pred)))
print("Classification report: {}".format(classification_report(y_test,y_pred)))

"""**Grid Search CV**"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'criterion': [rf_randomcv.best_params_['criterion']],
    'max_depth': [rf_randomcv.best_params_['max_depth']],
    'max_features': [rf_randomcv.best_params_['max_features']],
    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'],
                         rf_randomcv.best_params_['min_samples_leaf']+2,
                         rf_randomcv.best_params_['min_samples_leaf'] + 4],
    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,
                          rf_randomcv.best_params_['min_samples_split'] - 1,
                          rf_randomcv.best_params_['min_samples_split'],
                          rf_randomcv.best_params_['min_samples_split'] +1,
                          rf_randomcv.best_params_['min_samples_split'] + 2],
    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100,
                     rf_randomcv.best_params_['n_estimators'],
                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]
}

print(param_grid)

rf=RandomForestClassifier()
grid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)
grid_search.fit(X_train,y_train)

grid_search.best_estimator_

best_grid=grid_search.best_estimator_

best_grid

y_pred=best_grid.predict(X_test)
print(confusion_matrix(y_test,y_pred))
print("Accuracy Score {}".format(accuracy_score(y_test,y_pred)))
print("Classification report: {}".format(classification_report(y_test,y_pred)))

"""#**eXplainable AI (LIME and SHAP)**

LIME
"""

!pip install lime

import lime
import lime.lime_tabular

from sklearn.ensemble import RandomForestClassifier
rf_classifier= RandomForestClassifier(n_estimators=150)
rf_classifier.fit(X_train,y_train)
y_pred_rf= rf_classifier.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(confusion_matrix(y_test,y_pred_rf))
print(accuracy_score(y_test,y_pred_rf))
print(classification_report(y_test,y_pred_rf))

from lime.lime_tabular import LimeTabularExplainer
X= pd.DataFrame(X_test)
y= pd.DataFrame(y_test)
explainer = LimeTabularExplainer(X_train.values, feature_names=X_train.columns.values.tolist(), class_names=['GROUP'], mode='regression')
exp = explainer.explain_instance(X_train.values[100], rf_classifier.predict, num_features=5)
exp.as_pyplot_figure()
from matplotlib import pyplot as plt
plt.tight_layout()

exp.show_in_notebook(show_table=True)

"""**SHAP**"""

!pip install shapash

model2 = RandomForestClassifier(max_depth=5, random_state=42, n_estimators=12)
model2=model.fit(X_train, y_train)
rf_y_pred = model2.predict(X_test)

fi=pd.DataFrame({'Feature': X_train.columns, 'Importance': model.feature_importances_})
fi.sort_values(by='Importance',ascending=False,ignore_index=True)

import numpy as np
import matplotlib.pyplot as plt


# creating the dataset
data = {'CDR':0.568320, 'MMSE':0.203684, 'nWBV':0.102335,
		'Age':0.048443,'eTIV':0.039123,'ASF':0.038095}
features = list(data.keys())
importance = list(data.values())

fig = plt.figure(figsize = (10, 5))

# creating the bar plot
plt.bar(features, importance, color ='maroon',
		width = 0.4)

plt.xlabel("FEATURES")
plt.ylabel("IMPORTANCE")
plt.title("Feature Importance")
plt.show()

from shapash.explainer.smart_explainer import SmartExplainer
xpl = SmartExplainer(model2)
xpl.compile(x=X_test)
xpl.plot.features_importance()

import random
subset = random.choices(X_test.index, k =50)
xpl.plot.features_importance(selection=subset)

xpl.plot.local_plot(index=random.choice(X_test.index))

import shap
row_to_show = 5
data_for_prediction = X_test.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired
data_for_prediction_array = data_for_prediction.values.reshape(1, -1)
model.predict_proba(data_for_prediction_array)

# use Kernel SHAP to explain test set predictions
k_explainer = shap.KernelExplainer(model2.predict_proba, X_train)
k_shap_values = k_explainer.shap_values(data_for_prediction)
shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)